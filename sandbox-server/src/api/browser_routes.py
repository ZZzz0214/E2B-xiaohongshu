"""
E2B æµè§ˆå™¨è‡ªåŠ¨åŒ– API - æ¸…ç†ç‰ˆæœ¬
åªä¿ç•™æ ¸å¿ƒæ¥å£ï¼šautomation å’Œ batch_process_from_database
"""
from fastapi import APIRouter, HTTPException
from typing import Dict, Any, List, Union
import logging
import asyncio

from core.sandbox_manager import sandbox_manager
from models.request_models import BatchBrowserRequest, BatchProcessFromDatabaseRequest
from models.llm_models import (
    PostClassificationItem, PostClassificationRequest,
    CommentAnalysisRequest,
    CustomerInsightsItem, convert_customer_insight_to_db_format
)
from utils.xiaohongshu_data_cleaner import XiaohongshuDataCleaner
from utils.api_response_builder import ApiResponseBuilder, ApiTimer
from xiaohongshuDataStorage import storage_service, post_repository
from xiaohongshuDataStorage.classification_repository import classification_repository
from xiaohongshuDataStorage.ai_analysis_repository import ai_analysis_repository
from xiaohongshuDataStorage.customer_insights_repository import customer_insights_repository

logger = logging.getLogger(__name__)
browser_router = APIRouter()

# ==================== æ ¸å¿ƒAPI ====================

@browser_router.post("/automation", response_model=Dict[str, Any])
async def browser_automation(request: BatchBrowserRequest):
    """
    æµè§ˆå™¨è‡ªåŠ¨åŒ–ä¸»æ¥å£ - E2Bäº‘ç«¯ç‰ˆæœ¬
    è‡ªåŠ¨åˆ›å»ºE2Bæ²™ç›’å¹¶æ‰§è¡Œæµè§ˆå™¨æ“ä½œ
    æ”¯æŒå•æ“ä½œå’Œæ‰¹é‡æ“ä½œä¸¤ç§æ ¼å¼
    """
    # ğŸš€ ä½¿ç”¨æ–°çš„æ—¶é—´ç®¡ç†å™¨
    timer = ApiTimer.start()
    
    try:
        # ğŸ¯ ä½¿ç”¨pydanticæ¨¡å‹éªŒè¯å’Œè§£ææ“ä½œæ•°æ®
        operations = request.get_operations_list()
        
        if not operations:
            raise HTTPException(
                status_code=400,
                detail="è¯·æ±‚æ ¼å¼é”™è¯¯ï¼šå¿…é¡»åŒ…å«'action'æˆ–'operations'å­—æ®µ"
            )
        
        logger.info(f"ğŸš€ å¼€å§‹æµè§ˆå™¨è‡ªåŠ¨åŒ–: {len(operations)} ä¸ªæ“ä½œ")
        
        # 1. è·å–æˆ–åˆ›å»ºE2Bæ²™ç›’
        sandbox_result = await sandbox_manager.get_or_create_sandbox(request.persistent_id)
        
        if not sandbox_result["success"]:
            return {
                "success": False,
                "message": f"æ²™ç›’åˆ›å»ºå¤±è´¥: {sandbox_result.get('message', 'Unknown error')}",
                "execution_time": timer.elapsed()
            }
        
        persistent_id = sandbox_result["persistent_id"]
        e2b_sandbox_id = sandbox_result["e2b_sandbox_id"]
        vnc_access = sandbox_result["vnc_access"]
        
        logger.info(f"âœ… æ²™ç›’å‡†å¤‡å°±ç»ª: {e2b_sandbox_id}")
        
        # 2. åœ¨E2Bæ²™ç›’ä¸­æ‰§è¡Œæµè§ˆå™¨æ“ä½œ
        execution_result = await sandbox_manager.execute_browser_operations(
            persistent_id, operations
        )
        
        # 3. åˆå¹¶ç»“æœ
        final_result = {
            **execution_result,
            "persistent_id": persistent_id,
            "e2b_sandbox_id": e2b_sandbox_id,
            "vnc_access": vnc_access,
            "total_execution_time": timer.elapsed(),
            "sandbox_created": not sandbox_result.get("existing", False)
        }
        
        # 4. æ•°æ®æ¸…æ´— (é’ˆå¯¹å°çº¢ä¹¦æ•°æ®)
        if XiaohongshuDataCleaner.should_clean(final_result):
            logger.info("ğŸ§¹ å¼€å§‹æ¸…æ´—å°çº¢ä¹¦æ•°æ®...")
            final_result = XiaohongshuDataCleaner.clean_batch_result(final_result)
            
        # 5. æ•°æ®å­˜å‚¨ (å­˜å‚¨æ¸…æ´—åçš„å°çº¢ä¹¦æ•°æ®)
        try:
            logger.info("ğŸ’¾ å¼€å§‹å­˜å‚¨å°çº¢ä¹¦æ•°æ®åˆ°æ•°æ®åº“...")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç”¨æˆ·ä¸ªäººä¿¡æ¯æ•°æ®
            if 'user_profile' in final_result and final_result.get('extraction_type') == 'user_profile':
                logger.info("ğŸ§‘ æ£€æµ‹åˆ°ç”¨æˆ·ä¸ªäººä¿¡æ¯æ•°æ®ï¼Œä½¿ç”¨ç”¨æˆ·ä¿¡æ¯å­˜å‚¨...")
                
                # è·å–æ¥æºä¿¡æ¯ï¼ˆä»è¯·æ±‚æˆ–ç»“æœä¸­ï¼‰
                source_post_id = request.source_post_id or final_result.get('source_post_id')
                source_comment_id = request.source_comment_id or final_result.get('source_comment_id')
                
                # å¦‚æœæ²¡æœ‰æä¾›æ¥æºä¿¡æ¯ï¼Œå°è¯•ä»ç”¨æˆ·å¤´åƒç‚¹å‡»æ“ä½œä¸­è·å–
                if not source_post_id and not source_comment_id:
                    # ç¡®ä¿ operations å­˜åœ¨ä¸”ä¸ä¸ºç©º
                    if request.operations:
                        for operation in request.operations:
                            if operation.action in ['xiaohongshu_click_author_avatar', 'click_author_avatar']:
                                params = operation.params or {}
                                source_post_id = params.get('source_post_id')
                                source_comment_id = params.get('source_comment_id')
                                break
                
                storage_result = storage_service.store_user_profile_data(
                    final_result.get('user_profile', {}), 
                    source_post_id, 
                    source_comment_id
                )
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¸–å­è¯¦æƒ…æ•°æ®ï¼ˆåŒ…å«author_postå’Œcommentsï¼‰
            elif 'author_post' in final_result and 'comments' in final_result:
                logger.info("ğŸ“ æ£€æµ‹åˆ°å¸–å­è¯¦æƒ…æ•°æ®ï¼Œä½¿ç”¨ç»Ÿä¸€å†…å®¹è¡¨å­˜å‚¨...")
                storage_result = storage_service.store_post_detail_data(final_result)
            else:
                # ä½¿ç”¨åŸæœ‰çš„æ‰¹é‡å¸–å­å­˜å‚¨æ–¹æ³•
                logger.info("ğŸ“„ æ£€æµ‹åˆ°æ‰¹é‡å¸–å­æ•°æ®ï¼Œä½¿ç”¨ä¼ ç»Ÿå­˜å‚¨æ–¹æ³•...")
                storage_result = storage_service.store_api_response_data(final_result)
            
            # è®°å½•æ•°æ®å­˜å‚¨å®Œæˆæ—¥å¿—ï¼ˆæ•°æ®æ¸…æ´—å™¨å·²ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼‰
            posts_data = final_result.get('posts', [])
            if posts_data:
                logger.info(f"ğŸ¯ æ•°æ®å­˜å‚¨å®Œæˆï¼ŒDifyå¯è·å– {len(posts_data)} ä¸ªæ¸…æ´—åå¸–å­")
            
            # ç»Ÿä¸€å¤„ç†å­˜å‚¨ç»“æœæ ¼å¼
            if 'user_profile' in final_result and final_result.get('extraction_type') == 'user_profile':
                # ç”¨æˆ·ä¿¡æ¯å­˜å‚¨ç»“æœæ ¼å¼
                final_result["storage_result"] = {
                    "success": storage_result["success"],
                    "message": storage_result["message"],
                    "user_id": storage_result.get("user_id"),
                    "posts_count": storage_result.get("posts_count", 0),
                    "stored_at": storage_result.get("stored_at")
                }
            else:
                # å¸–å­/è¯„è®ºæ•°æ®å­˜å‚¨ç»“æœæ ¼å¼
                final_result["storage_result"] = {
                    "success": storage_result["success"],
                    "message": storage_result["message"],
                    "stats": storage_result.get("stats", {}),
                    "stored_at": storage_result.get("stored_at")
                }
            logger.info(f"ğŸ’¾ æ•°æ®å­˜å‚¨å®Œæˆ: {storage_result['message']}")
        except Exception as e:
            logger.error(f"ğŸ’¾ æ•°æ®å­˜å‚¨å¤±è´¥: {e}")
            final_result["storage_result"] = {
                "success": False, 
                "error": str(e),
                "message": "æ•°æ®å­˜å‚¨å¤±è´¥"
            }
        
        # 6. å¯é€‰çš„è‡ªåŠ¨æ¸…ç†
        if request.auto_cleanup and final_result.get("success", False):
            logger.info("ğŸ§¹ è‡ªåŠ¨æ¸…ç†æ²™ç›’...")
            cleanup_result = await sandbox_manager.cleanup_sandbox(persistent_id)
            final_result["cleanup_result"] = cleanup_result
        
        logger.info(f"ğŸ¯ è‡ªåŠ¨åŒ–å®Œæˆï¼Œæ€»è€—æ—¶: {final_result['total_execution_time']:.2f}ç§’")
        return final_result
        
    except HTTPException:
        # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
        raise
    except Exception as e:
        logger.error(f"âŒ è‡ªåŠ¨åŒ–å¤±è´¥: {e}")
        return ApiResponseBuilder.build_error_response(
            error_msg=f"æµè§ˆå™¨è‡ªåŠ¨åŒ–å¤±è´¥: {str(e)}",
            timer_or_start_time=timer,
            total_execution_time=timer.elapsed()
        )

# ==================== æ•°æ®åº“é©±åŠ¨çš„æ‰¹é‡å¤„ç†åŠŸèƒ½ ====================

async def get_unprocessed_posts_from_database(condition: str, limit: int) -> List[Dict]:
    """ä»æ•°æ®åº“æŸ¥è¯¢å¾…å¤„ç†çš„å¸–å­åˆ—è¡¨"""
    try:
        logger.info(f"ğŸ” æŸ¥è¯¢æ•°æ®åº“ï¼šcondition='{condition}', limit={limit}")
        
        # ä½¿ç”¨ç°æœ‰çš„post_repositoryæŸ¥è¯¢æ•°æ®åº“
        posts = post_repository.get_posts_by_condition(condition, limit)
        
        logger.info(f"ğŸ“Š æŸ¥è¯¢ç»“æœï¼šæ‰¾åˆ° {len(posts)} ä¸ªå¾…å¤„ç†å¸–å­")
        return posts
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        return []

async def mark_posts_as_processed(post_ids: List[str]) -> bool:
    """æ ‡è®°å¸–å­ä¸ºå·²å¤„ç†"""
    try:
        for post_id in post_ids:
            post_repository.update_post_status(post_id, {"detail_extracted": True})
        logger.info(f"âœ… å·²æ ‡è®° {len(post_ids)} ä¸ªå¸–å­ä¸ºå·²å¤„ç†")
        return True
    except Exception as e:
        logger.error(f"âŒ æ ‡è®°å¸–å­çŠ¶æ€å¤±è´¥: {e}")
        return False

@browser_router.post("/batch_process_from_database")
async def batch_process_posts_from_database(request: BatchProcessFromDatabaseRequest):
    """
    æ•°æ®åº“é©±åŠ¨çš„æ‰¹é‡å¸–å­å¤„ç†å·¥ä½œæµ
    å‰ææ¡ä»¶ï¼šæ²™ç›’å·²è¿è¡Œï¼Œæµè§ˆå™¨å·²æ‰“å¼€ï¼Œå·²åœ¨å°çº¢ä¹¦å…³é”®è¯é¡µé¢
    åŠŸèƒ½ï¼šä»æ•°æ®åº“æŸ¥è¯¢å¾…å¤„ç†å¸–å­ï¼Œé€ä¸ªç‚¹å‡»æå–è¯¦ç»†ä¿¡æ¯
    """
    # ğŸš€ ä½¿ç”¨æ–°çš„æ—¶é—´ç®¡ç†å™¨
    timer = ApiTimer.start()
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹æ•°æ®åº“é©±åŠ¨çš„æ‰¹é‡å¤„ç†: limit={request.limit}")
        
        # 1. éªŒè¯æ²™ç›’æ˜¯å¦å­˜åœ¨
        persistent_id = request.persistent_id
        if persistent_id not in sandbox_manager.active_sandboxes:
            return {
                "success": False,
                "message": f"æ²™ç›’ {persistent_id} ä¸å­˜åœ¨ï¼Œè¯·å…ˆç¡®ä¿æ²™ç›’å·²åˆ›å»ºå¹¶è¿è¡Œ",
                "total_execution_time": timer.elapsed()
            }
        
        sandbox = sandbox_manager.active_sandboxes[persistent_id]
        e2b_sandbox_id = sandbox.sandbox_id
        vnc_access = {
            "vnc_web_url": f"https://6080-{e2b_sandbox_id}.e2b.app",
            "vnc_direct_url": f"vnc://5901-{e2b_sandbox_id}.e2b.app",
            "display": ":1",
            "note": "é€šè¿‡VNC Web URLå¯å®æ—¶è§‚å¯Ÿæµè§ˆå™¨æ“ä½œ"
        }
        
        logger.info(f"âœ… ä½¿ç”¨ç°æœ‰æ²™ç›’: {e2b_sandbox_id}")
        
        # 2. ä»æ•°æ®åº“æŸ¥è¯¢å¾…å¤„ç†å¸–å­
        posts = await get_unprocessed_posts_from_database(
            request.query_condition, 
            request.limit
        )
        
        if not posts:
            return {
                "success": False,
                "message": "æ²¡æœ‰æ‰¾åˆ°å¾…å¤„ç†çš„å¸–å­",
                "total_execution_time": timer.elapsed()
            }
        
        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(posts)} ä¸ªå¾…å¤„ç†å¸–å­")
        
        # 3. å…ˆæ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå¸–å­ï¼Œç¡®ä¿ç›®æ ‡å¸–å­åœ¨é¡µé¢ä¸Š
        logger.info("ğŸ“œ å¼€å§‹æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå¸–å­ï¼Œç¡®ä¿ç›®æ ‡å¸–å­å¯è§...")
        scroll_operations = [
            {
                "action": "xiaohongshu_auto_scroll",
                "params": {"max_scrolls": 20, "delay_between_scrolls": 1.5},
                "description": "æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå¸–å­"
            }
        ]
        
        scroll_result = await sandbox_manager.execute_browser_operations(
            persistent_id, scroll_operations
        )
        
        if scroll_result.get("success", False):
            scroll_data = scroll_result.get("data", {})
            total_loaded_posts = scroll_data.get("total_posts", 0)
            total_scrolls = scroll_data.get("total_scrolls", 0)
            logger.info(f"âœ… æ»šåŠ¨å®Œæˆï¼šåŠ è½½äº† {total_loaded_posts} ä¸ªå¸–å­ (æ»šåŠ¨ {total_scrolls} æ¬¡)")
        else:
            logger.warning(f"âš ï¸ é¡µé¢æ»šåŠ¨å¤±è´¥ï¼Œç»§ç»­å¤„ç†: {scroll_result.get('message', '')}")

        # 4. é€ä¸ªå¤„ç†å¸–å­ï¼ˆæ ¸å¿ƒå¾ªç¯ï¼‰
        processed_posts = []
        failed_posts = []
        
        for i, post in enumerate(posts):
            try:
                post_title = post.get('title', post.get('post_id', 'Unknown'))
                logger.info(f"ğŸ¯ å¤„ç†å¸–å­ {i+1}/{len(posts)}: {post_title}")
                
                # æ„é€ å•ä¸ªå¸–å­çš„å¤„ç†æ“ä½œåºåˆ—ï¼ˆå®Œæ•´é—­ç¯ï¼‰
                post_operations = [
                    {
                        "action": "xiaohongshu_click_post",
                        "params": {"title": post.get('title', '')},
                        "description": "æ­¥éª¤1: ç‚¹å‡»è¿›å…¥å¸–å­è¯¦æƒ…é¡µ"
                    },
                    {
                        "action": "xiaohongshu_expand_comments",
                        "params": {"max_attempts": 10},
                        "description": "æ­¥éª¤2: å±•å¼€æ‰€æœ‰è¯„è®º"
                    },
                    {
                        "action": "xiaohongshu_extract_comments",
                        "params": {"include_replies": True},
                        "description": "æ­¥éª¤3: æå–å¸–å­å†…å®¹å’Œæ‰€æœ‰è¯„è®ºæ•°æ®"
                    },
                    {
                        "action": "xiaohongshu_close_post",
                        "params": {},
                        "description": "æ­¥éª¤4: å…³é—­å¸–å­è¯¦æƒ…é¡µï¼Œè¿”å›å¸–å­åˆ—è¡¨"
                    }
                ]
                
                # æ‰§è¡Œæ“ä½œåºåˆ—
                post_result = await sandbox_manager.execute_browser_operations(
                    persistent_id, post_operations
                )
                
                logger.info(f"ğŸ” å¸–å­æ“ä½œç»“æœ: success={post_result.get('success', False)}, message='{post_result.get('message', '')}'")
                
                if post_result.get("success", False):
                    logger.info(f"ğŸ“‹ å¼€å§‹æ•°æ®å¤„ç†æµç¨‹: {post_title}")
                    
                    # æ•°æ®æ¸…æ´—æ£€æŸ¥
                    should_clean = XiaohongshuDataCleaner.should_clean(post_result)
                    logger.info(f"ğŸ§¹ æ•°æ®æ¸…æ´—æ£€æŸ¥: should_clean={should_clean}")
                    
                    if should_clean:
                        cleaned_result = XiaohongshuDataCleaner.clean_batch_result(post_result)
                        logger.info(f"ğŸ§¹ æ•°æ®æ¸…æ´—å®Œæˆ: author_post={('author_post' in cleaned_result)}, comments={('comments' in cleaned_result)}")
                        
                        # å­˜å‚¨è¯¦ç»†æ•°æ®
                        if 'author_post' in cleaned_result and 'comments' in cleaned_result:
                            logger.info(f"ğŸ“Š æ•°æ®æ ¼å¼æ­£ç¡®ï¼Œå¼€å§‹å­˜å‚¨: {post_title}")
                            storage_result = storage_service.store_post_detail_data(cleaned_result)
                            logger.info(f"ğŸ’¾ å­˜å‚¨ç»“æœ: success={storage_result.get('success', False)}")
                            
                            if storage_result["success"]:
                                # æ ‡è®°ä¸ºå·²å¤„ç†
                                await mark_posts_as_processed([post.get('post_id')])
                                processed_posts.append(post.get('post_id'))
                                logger.info(f"âœ… å¸–å­å¤„ç†æˆåŠŸ: {post_title}")
                            else:
                                failed_posts.append(post.get('post_id'))
                                logger.error(f"âŒ æ•°æ®å­˜å‚¨å¤±è´¥: {post_title} - {storage_result.get('message', '')}")
                        else:
                            failed_posts.append(post.get('post_id'))
                            logger.error(f"âŒ æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {post_title} - author_postå­˜åœ¨: {'author_post' in cleaned_result}, commentså­˜åœ¨: {'comments' in cleaned_result}")
                    else:
                        failed_posts.append(post.get('post_id'))
                        logger.error(f"âŒ æ•°æ®æ¸…æ´—æ£€æŸ¥å¤±è´¥: {post_title}")
                        logger.error(f"âŒ post_resultå†…å®¹é¢„è§ˆ: {str(post_result)[:200]}...")
                else:
                    failed_posts.append(post.get('post_id'))
                    logger.error(f"âŒ å¸–å­æ“ä½œå¤±è´¥: {post_title} - {post_result.get('message', '')}")
                    logger.error(f"âŒ post_resultè¯¦æƒ…: {str(post_result)[:300]}...")
                
                # å¤„ç†å®Œä¸€ä¸ªå¸–å­åï¼Œå¯¼èˆªå›åˆ—è¡¨é¡µï¼ˆé‡è¦ï¼ï¼‰
                if request.base_url:
                    # å¦‚æœæä¾›äº†åŸºç¡€URLï¼Œç›´æ¥å¯¼èˆªå›å»ï¼ˆæ¨èï¼‰
                    back_operation = [{
                        "action": "navigate",
                        "params": {"url": request.base_url},
                        "description": "è¿”å›å°çº¢ä¹¦å…³é”®è¯é¡µé¢"
                    }]
                else:
                    # å¦åˆ™å°è¯•æµè§ˆå™¨åé€€
                    back_operation = [{
                        "action": "execute_script",
                        "params": {"script": "window.history.back(); await new Promise(resolve => setTimeout(resolve, 1000));"},
                        "description": "æµè§ˆå™¨åé€€åˆ°åˆ—è¡¨é¡µ"
                    }]
                await sandbox_manager.execute_browser_operations(persistent_id, back_operation)
                
                # å¸–å­é—´å»¶è¿Ÿ
                if request.delay_between_posts > 0:
                    await asyncio.sleep(request.delay_between_posts)
                    
                # å®šæœŸè¾“å‡ºè¿›åº¦
                if (i + 1) % 10 == 0:
                    logger.info(f"ğŸ“Š è¿›åº¦æ›´æ–°: å·²å¤„ç† {i+1}/{len(posts)} ä¸ªå¸–å­ï¼ŒæˆåŠŸ {len(processed_posts)} ä¸ª")
                    
            except Exception as e:
                failed_posts.append(post.get('post_id'))
                logger.error(f"âŒ å¤„ç†å¸–å­å¼‚å¸¸: {post_title} - {str(e)}")
        
        # 4. æ„å»ºæœ€ç»ˆç»“æœ
        total_execution_time = timer.elapsed()
        success_rate = len(processed_posts) / len(posts) if posts else 0
        
        final_result = {
            "success": len(processed_posts) > 0,
            "message": f"æ‰¹é‡å¤„ç†å®Œæˆ: {len(processed_posts)}/{len(posts)} ä¸ªå¸–å­æˆåŠŸå¤„ç†",
            "statistics": {
                "total_posts": len(posts),
                "processed_posts": len(processed_posts),
                "failed_posts": len(failed_posts),
                "success_rate": f"{success_rate:.2%}"
            },
            "processed_post_ids": processed_posts,
            "failed_post_ids": failed_posts,
            "execution_details": {
                "total_execution_time": total_execution_time,
                "average_time_per_post": total_execution_time / len(posts) if posts else 0,
                "persistent_id": persistent_id,
                "e2b_sandbox_id": e2b_sandbox_id,
                "vnc_access": vnc_access
            }
        }
        
        logger.info(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ: {final_result['message']}, æ€»è€—æ—¶: {total_execution_time:.2f}ç§’")
        return final_result
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡å¤„ç†å¼‚å¸¸: {e}")
        return ApiResponseBuilder.build_error_response(
            error_msg=f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}",
            timer_or_start_time=timer,
            total_execution_time=timer.elapsed()
        )

# ==================== LLMåˆ†æç»“æœå­˜å‚¨API ====================

@browser_router.post("/store-post-classification")
async def store_post_classification(request: Union[PostClassificationRequest, List[PostClassificationItem]]):
    """
    å­˜å‚¨LLMå¸–å­åˆ†ç±»ç»“æœ - æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. åŒ…è£…æ ¼å¼ï¼š{"post_classification": [...]}
    2. ç›´æ¥æ•°ç»„æ ¼å¼ï¼š[{...}, {...}, ...]
    """
    # ğŸš€ ä½¿ç”¨æ–°çš„æ—¶é—´ç®¡ç†å™¨
    timer = ApiTimer.start()
    
    try:
        # ğŸ¯ åˆ¤æ–­è¯·æ±‚æ ¼å¼å¹¶æå–å¸–å­åˆ—è¡¨
        if isinstance(request, list):
            # ç›´æ¥æ•°ç»„æ ¼å¼
            post_items = request
            logger.info(f"ğŸ“ å¼€å§‹æ‰¹é‡å­˜å‚¨å¸–å­åˆ†ç±»ç»“æœ (ç›´æ¥æ•°ç»„æ ¼å¼): {len(post_items)} ä¸ªå¸–å­")
        else:
            # åŒ…è£…æ ¼å¼
            post_items = request.post_classification
            logger.info(f"ğŸ“ å¼€å§‹æ‰¹é‡å­˜å‚¨å¸–å­åˆ†ç±»ç»“æœ (åŒ…è£…æ ¼å¼): {len(post_items)} ä¸ªå¸–å­")
        
        # è½¬æ¢APIæ•°æ®ä¸ºæ•°æ®åº“æ ¼å¼
        classifications = []
        for post in post_items:
            classification_data = {
                'post_id': post.post_id,
                'classification': post.classification,
                'confidence_score': post.confidence_score,
                'reasoning': post.reasoning,
                'commercial_value': post.commercial_value,
                'need_ai_analysis': post.need_ai_analysis
            }
            classifications.append(classification_data)
        
        # æ‰§è¡ŒçœŸå®çš„æ•°æ®åº“å­˜å‚¨
        storage_result = classification_repository.batch_insert_classifications(classifications)
        
        # ğŸ¯ ä½¿ç”¨æ—¶é—´ç®¡ç†å™¨æ„å»ºå“åº”
        result = ApiResponseBuilder.build_batch_api_response(
            storage_result=storage_result,
            timer_or_start_time=timer,
            success_template="æ‰¹é‡å¸–å­åˆ†ç±»å­˜å‚¨å®Œæˆ: {stored_count}/{total_count} ä¸ªæˆåŠŸ",
            failure_template="æ‰¹é‡å¸–å­åˆ†ç±»å­˜å‚¨å¤±è´¥: {failed_count}/{total_count} ä¸ªå¤±è´¥",
            extra_data={"total_posts": len(post_items)}
        )
        
        logger.info(f"âœ… æ‰¹é‡å¸–å­åˆ†ç±»å­˜å‚¨å®Œæˆ: {storage_result['stored_count']} ä¸ªæˆåŠŸ, {storage_result['failed_count']} ä¸ªå¤±è´¥")
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡å¸–å­åˆ†ç±»å­˜å‚¨å¼‚å¸¸: {e}")
        
        # è®¡ç®—å¸–å­æ€»æ•°ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰
        total_posts = 0
        try:
            if isinstance(request, list):
                total_posts = len(request)
            elif hasattr(request, 'post_classification') and request.post_classification:
                total_posts = len(request.post_classification)
        except Exception:
            total_posts = 0
        
        return ApiResponseBuilder.build_error_response(
            error_msg=f"æ‰¹é‡å¸–å­åˆ†ç±»å­˜å‚¨å¤±è´¥: {str(e)}",
            timer_or_start_time=timer,
            total_posts=total_posts
        )


@browser_router.post("/store-comment-analysis")
async def store_comment_analysis(request: CommentAnalysisRequest):
    """
    å­˜å‚¨LLMè¯„è®ºåˆ†æç»“æœï¼ˆèŠ‚ç‚¹3è¾“å‡ºï¼‰
    æ¥æ”¶LLMåŒ…è£…æ ¼å¼ï¼š{"comment_analysis": [...]}
    """
    # ğŸš€ ä½¿ç”¨æ–°çš„æ—¶é—´ç®¡ç†å™¨
    timer = ApiTimer.start()
    
    try:
        logger.info(f"ğŸ“Š å¼€å§‹å­˜å‚¨è¯„è®ºåˆ†æç»“æœ: {len(request.comment_analysis)} æ¡è¯„è®º")
        
        # è½¬æ¢APIæ•°æ®ä¸ºæ•°æ®åº“æ ¼å¼
        analyses = []
        for comment in request.comment_analysis:
            analysis_data = {
                'comment_id': comment.comment_id,
                'user_id': comment.user_id,
                'username': comment.username,
                'content': comment.content,
                'is_valid': comment.is_valid,
                'intent_type': comment.intent_type,
                'intent_level': comment.intent_level,
                'sentiment': comment.sentiment,
                'confidence': comment.confidence,
                'key_features': {
                    'product_category': comment.key_features.product_category,
                    'functional_needs': comment.key_features.functional_needs,
                    'user_profile': comment.key_features.user_profile,
                    'size_info': comment.key_features.size_info
                },
                'business_value': comment.business_value,
                'follow_up_action': comment.follow_up_action
            }
            analyses.append(analysis_data)
        
        # æ‰§è¡ŒçœŸå®çš„æ•°æ®åº“å­˜å‚¨
        storage_result = ai_analysis_repository.batch_insert_comment_analysis(analyses)
        
        # ğŸ¯ ä½¿ç”¨æ—¶é—´ç®¡ç†å™¨æ„å»ºå“åº”
        result = ApiResponseBuilder.build_batch_api_response(
            storage_result=storage_result,
            timer_or_start_time=timer,
            success_template="è¯„è®ºåˆ†æç»“æœå­˜å‚¨å®Œæˆ: {stored_count}/{total_count} æ¡æˆåŠŸ",
            failure_template="è¯„è®ºåˆ†æç»“æœå­˜å‚¨å¤±è´¥: {failed_count}/{total_count} æ¡å¤±è´¥",
            extra_data={"total_comments": storage_result['total_count']}
        )
        
        logger.info(f"âœ… è¯„è®ºåˆ†æå­˜å‚¨å®Œæˆ: {storage_result['stored_count']} æ¡æˆåŠŸ, {storage_result['failed_count']} æ¡å¤±è´¥")
        return result
        
    except Exception as e:
        logger.error(f"âŒ è¯„è®ºåˆ†æå­˜å‚¨å¼‚å¸¸: {e}")
        return ApiResponseBuilder.build_error_response(
            error_msg=f"è¯„è®ºåˆ†æå­˜å‚¨å¤±è´¥: {str(e)}",
            timer_or_start_time=timer,
            total_comments=len(request.comment_analysis) if request and request.comment_analysis else 0
        )

# ==================== å®¢æˆ·æ´å¯Ÿåˆ†æå­˜å‚¨API ====================

@browser_router.post("/store-customer-insights")
async def store_customer_insights(insights: Union[CustomerInsightsItem, List[CustomerInsightsItem]]):
    """
    å­˜å‚¨å®¢æˆ·æ´å¯Ÿåˆ†æç»“æœ - æ”¯æŒå•ä¸ªæˆ–æ‰¹é‡å¤„ç†
    
    å‚æ•°æ ¼å¼:
    - å•ä¸ªå®¢æˆ·: CustomerInsightsItem å¯¹è±¡
    - å¤šä¸ªå®¢æˆ·: List[CustomerInsightsItem] æ•°ç»„
    
    æ¥æ”¶å¤§æ¨¡å‹åˆ†æçš„å®¢æˆ·æ´å¯Ÿæ•°æ®å¹¶å­˜å‚¨åˆ°xiaohongshu_customer_insightsè¡¨
    """
    timer = ApiTimer.start()
    
    try:
        # ğŸ¯ åˆ¤æ–­æ˜¯å•ä¸ªè¿˜æ˜¯æ‰¹é‡å¤„ç†
        is_batch = isinstance(insights, list)
        insights_list = insights if is_batch else [insights]
        
        logger.info(f"ğŸ§  å¼€å§‹å­˜å‚¨å®¢æˆ·æ´å¯Ÿ: {'æ‰¹é‡' if is_batch else 'å•ä¸ª'} - {len(insights_list)} ä¸ªç”¨æˆ·")
        
        if is_batch:
            # ğŸ“¦ æ‰¹é‡å¤„ç†é€»è¾‘
            batch_data = [
                {
                    'user_id': insight.user_id,
                    'insights_data': convert_customer_insight_to_db_format(insight)
                }
                for insight in insights_list
            ]
            
            # æ‰§è¡Œæ‰¹é‡å­˜å‚¨
            batch_result = customer_insights_repository.batch_insert_insights(batch_data)
            
            # æ„å»ºæ‰¹é‡å“åº”
            result = ApiResponseBuilder.build_batch_insights_api_response(
                batch_result=batch_result,
                timer_or_start_time=timer
            )
            
            logger.info(f"âœ… æ‰¹é‡å®¢æˆ·æ´å¯Ÿå­˜å‚¨å®Œæˆ: {batch_result['success_count']} ä¸ªç”¨æˆ·æˆåŠŸ, {batch_result['error_count']} ä¸ªç”¨æˆ·å¤±è´¥")
            
        else:
            # ğŸ‘¤ å•ä¸ªå¤„ç†é€»è¾‘
            single_insight = insights_list[0]
            
            # è½¬æ¢APIæ•°æ®ä¸ºæ•°æ®åº“æ ¼å¼
            insights_data = convert_customer_insight_to_db_format(single_insight)
            
            # æ‰§è¡Œå•ä¸ªå­˜å‚¨
            storage_result = customer_insights_repository.insert_customer_insights(
                single_insight.user_id, 
                insights_data
            )
            
            # æ„å»ºå•ä¸ªå“åº”
            result = ApiResponseBuilder.build_insights_api_response(
                storage_result=storage_result,
                timer_or_start_time=timer,
                user_id=single_insight.user_id,
                tags_count=len(single_insight.primary_tags),
                intent_level=single_insight.latest_intent_level,
                intent_type=single_insight.latest_intent_type
            )
            
            logger.info(f"âœ… å®¢æˆ·æ´å¯Ÿå­˜å‚¨å®Œæˆ: ç”¨æˆ· {single_insight.user_id}, æ ‡ç­¾æ¦‚è§ˆ: {storage_result.get('tags_overview', '')}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ å®¢æˆ·æ´å¯Ÿå­˜å‚¨å¼‚å¸¸: {e}")
        
        # é”™è¯¯å“åº”å¤„ç†
        if isinstance(insights, list):
            return ApiResponseBuilder.build_error_response(
                error_msg=f"æ‰¹é‡å®¢æˆ·æ´å¯Ÿå­˜å‚¨å¤±è´¥: {str(e)}",
                timer_or_start_time=timer,
                total_users=len(insights)
            )
        else:
            return ApiResponseBuilder.build_error_response(
                error_msg=f"å®¢æˆ·æ´å¯Ÿå­˜å‚¨å¤±è´¥: {str(e)}",
                timer_or_start_time=timer,
                user_id=insights.user_id if insights else "unknown"
            )